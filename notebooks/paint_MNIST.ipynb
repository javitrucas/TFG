{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd57a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javitrucas/miniconda3/envs/tfg/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/javitrucas/miniconda3/envs/tfg/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m         avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader_train)\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÉpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Cell 5: Evaluación en test set\u001b[39;00m\n\u001b[32m     74\u001b[39m model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m bag_label = bag_label.to(device)\n\u001b[32m     61\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m output, attn = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m loss = criterion(output.squeeze(-\u001b[32m1\u001b[39m), bag_label)\n\u001b[32m     64\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TFG/scripts/MNIST/MNISTmodel.py:53\u001b[39m, in \u001b[36mMILModel.forward\u001b[39m\u001b[34m(self, bag_data, mask, adj_mat)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Colapsamos batch_size y bag_size para procesar cada instancia por separado\u001b[39;00m\n\u001b[32m     52\u001b[39m instances = bag_data.view(batch_size * max_bag_size, C, H, W)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ahora features tendrá forma: [batch_size * bag_size, feature_dim]\u001b[39;00m\n\u001b[32m     54\u001b[39m features = features.view(batch_size, max_bag_size, -\u001b[32m1\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooling_type == \u001b[33m'\u001b[39m\u001b[33mattention\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TFG/scripts/MNIST/MNISTmodel.py:13\u001b[39m, in \u001b[36mCNNFeatureExtractor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m     x = F.max_pool2d(x, \u001b[32m2\u001b[39m)\n\u001b[32m     15\u001b[39m     x = F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tfg/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar el directorio raíz del proyecto a sys.path\n",
    "project_root = \"/home/javitrucas/TFG\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Cell 1: Imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scripts.MNIST.MNISTMILDataset import MNISTMILDataset\n",
    "from scripts.MNIST.MNISTmodel import MILModel\n",
    "\n",
    "# Reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparámetros\n",
    "target_digit = 3       # Dígito objetivo para las bolsas\n",
    "bag_size = 10          # Número de instancias por bolsa\n",
    "num_epochs = 10        # Número de épocas\n",
    "a_learning_rate = 1e-3  # Tasa de aprendizaje\n",
    "batch_size = 1         # Tamaño de lote\n",
    "pooling_type = 'mean'  # 'attention', 'mean' o 'max'\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cell 2: Dataset y DataLoaders\n",
    "full_train_dataset = MNISTMILDataset(subset='train', bag_size=bag_size, obj_label=target_digit)\n",
    "test_dataset = MNISTMILDataset(subset='test', bag_size=bag_size, obj_label=target_digit)\n",
    "\n",
    "# División train/validation\n",
    "total_train = len(full_train_dataset)\n",
    "val_size = int(0.2 * total_train)\n",
    "train_size = total_train - val_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Cell 3: Modelo, criterio y optimizador\n",
    "model = MILModel(pooling_type=pooling_type).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=a_learning_rate)\n",
    "\n",
    "# Cell 4: Loop de entrenamiento\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        total_loss = 0.0\n",
    "        for bag, bag_label, inst_labels, lap in dataloader_train:\n",
    "            bag = bag.to(device)\n",
    "            bag_label = bag_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, attn = model(bag)\n",
    "            loss = criterion(output.squeeze(-1), bag_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader_train)\n",
    "        print(f\"Época {epoch}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "train()\n",
    "\n",
    "# Cell 5: Evaluación en test set\n",
    "model.eval()\n",
    "all_outputs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for bag, bag_label, inst_labels, lap in dataloader_test:\n",
    "        bag = bag.to(device)\n",
    "        bag_label = bag_label.to(device)\n",
    "        output, attn = model(bag)\n",
    "        all_outputs.append(output.cpu())\n",
    "        all_labels.append(bag_label.cpu())\n",
    "\n",
    "# Métricas\n",
    "torch_outputs = torch.cat(all_outputs).squeeze()\n",
    "torch_labels = torch.cat(all_labels)\n",
    "accuracy = ((torch_outputs > 0.5).float() == torch_labels).float().mean()\n",
    "print(f\"Accuracy en test: {accuracy:.4f}\")\n",
    "\n",
    "# Visualización de pesos de atención (si aplica)\n",
    "if pooling_type == 'attention':\n",
    "    import matplotlib.pyplot as plt\n",
    "    bag, _, _, _ = next(iter(dataloader_test))\n",
    "    _, attn_weights = model(bag.to(device))\n",
    "    attn_weights = attn_weights.cpu().numpy()[0]\n",
    "    plt.bar(range(len(attn_weights)), attn_weights)\n",
    "    plt.title('Pesos de atención para una bolsa de prueba')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495a8434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Época 1/10 - Train Loss: 0.2378, Val Loss: 0.1065, Val Acc: 0.9658\n",
      "Época 2/10 - Train Loss: 0.0846, Val Loss: 0.0663, Val Acc: 0.9783\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 10 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 243\u001b[39m\n\u001b[32m    240\u001b[39m     plt.show()\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# Cell 5: Evaluación en test set con visualizaciones\u001b[39;00m\n\u001b[32m    246\u001b[39m model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    219\u001b[39m             val_attn = val_attn.cpu().numpy()[\u001b[32m0\u001b[39m]\n\u001b[32m    221\u001b[39m             \u001b[38;5;66;03m# Visualizar la bolsa con pesos de atención\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m             \u001b[43mvisualize_bag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_bag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_inst_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_lap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mÉpoca \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m - Bolsa de validación (pred=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval_pred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    229\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Visualizar curvas de pérdida\u001b[39;00m\n\u001b[32m    232\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mvisualize_bag\u001b[39m\u001b[34m(bag, bag_label, inst_labels, lap, attn_weights, title, save_path)\u001b[39m\n\u001b[32m     86\u001b[39m im = ax.imshow(img, cmap=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     88\u001b[39m border_color = \u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inst_labels[i] == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m ax.set_title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlap\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, color=border_color)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Si hay pesos de atención, mostrarlos en el título\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 10 elements cannot be converted to Scalar"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar el directorio raíz del proyecto a sys.path\n",
    "project_root = \"/home/javitrucas/TFG\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Cell 1: Imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from scripts.MNIST.MNISTMILDataset import MNISTMILDataset\n",
    "from scripts.MNIST.MNISTmodel import MILModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparámetros\n",
    "target_digit = 3       # Dígito objetivo para las bolsas\n",
    "bag_size = 10          # Número de instancias por bolsa\n",
    "num_epochs = 10        # Número de épocas\n",
    "a_learning_rate = 1e-3  # Tasa de aprendizaje\n",
    "batch_size = 1         # Tamaño de lote\n",
    "pooling_type = 'attention'  # 'attention', 'mean' o 'max'\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cell 2: Dataset y DataLoaders\n",
    "full_train_dataset = MNISTMILDataset(subset='train', bag_size=bag_size, obj_label=target_digit)\n",
    "test_dataset = MNISTMILDataset(subset='test', bag_size=bag_size, obj_label=target_digit)\n",
    "\n",
    "# División train/validation\n",
    "total_train = len(full_train_dataset)\n",
    "val_size = int(0.2 * total_train)\n",
    "train_size = total_train - val_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=batch_size)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Cell 3: Modelo, criterio y optimizador\n",
    "model = MILModel(pooling_type=pooling_type).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=a_learning_rate)\n",
    "\n",
    "# Función para visualizar una bolsa y sus etiquetas\n",
    "def visualize_bag(bag, bag_label, inst_labels, lap=None, attn_weights=None, title=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualiza una bolsa de instancias MNIST con sus etiquetas y opcionalmente los pesos de atención.\n",
    "    \n",
    "    Args:\n",
    "        bag: Tensor con las imágenes (batch_size, bag_size, 1, 28, 28)\n",
    "        bag_label: Etiqueta de la bolsa (positiva o negativa)\n",
    "        inst_labels: Etiquetas de cada instancia\n",
    "        lap: Indices de los dígitos originales (para referencia)\n",
    "        attn_weights: Opcional, pesos de atención por instancia\n",
    "        title: Opcional, título para la visualización\n",
    "        save_path: Opcional, ruta para guardar la imagen\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    bag = bag.squeeze(0)  # Eliminar dimensión de batch\n",
    "    inst_labels = inst_labels.squeeze(0).numpy()\n",
    "    \n",
    "    # Título principal\n",
    "    if title is None:\n",
    "        title = f\"Bolsa {'POSITIVA' if bag_label.item() > 0.5 else 'NEGATIVA'} - Dígito objetivo: {target_digit}\"\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Visualizar cada instancia\n",
    "    for i in range(bag_size):\n",
    "        img = bag[i].squeeze().cpu().numpy()\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(img, cmap='gray')\n",
    "        \n",
    "        border_color = 'red' if inst_labels[i] == 1 else 'blue'\n",
    "        ax.set_title(f\"{lap[0][i].item()}\", color=border_color)\n",
    "        \n",
    "        # Si hay pesos de atención, mostrarlos en el título\n",
    "        if attn_weights is not None:\n",
    "            ax.set_xlabel(f\"Attn: {attn_weights[i]:.3f}\", fontsize=8)\n",
    "            # Añadir un borde coloreado según el peso de atención\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_linewidth(2)\n",
    "                # Color más intenso para mayores pesos de atención\n",
    "                intensity = min(1.0, attn_weights[i] * 3)\n",
    "                spine.set_edgecolor((1.0, 0.0, 0.0, intensity))\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Función para visualizar un mapa de calor basado en atención\n",
    "def visualize_attention_heatmap(bag, attn_weights, bag_label, prediction, save_path=None):\n",
    "    \"\"\"\n",
    "    Genera un mapa de calor superpuesto para visualizar la atención sobre una bolsa.\n",
    "    \n",
    "    Args:\n",
    "        bag: Tensor con las imágenes (bag_size, 1, 28, 28)\n",
    "        attn_weights: Pesos de atención normalizados\n",
    "        bag_label: Etiqueta real de la bolsa\n",
    "        prediction: Predicción del modelo para la bolsa\n",
    "        save_path: Opcional, ruta para guardar la imagen\n",
    "    \"\"\"\n",
    "    # Crear un grid para todas las instancias\n",
    "    bag = bag.squeeze(0)  # Eliminar dimensión de batch\n",
    "    grid_size = int(np.ceil(np.sqrt(bag_size)))\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Crear una imagen compuesta con todas las instancias\n",
    "    composite_img = np.zeros((grid_size * 28, grid_size * 28))\n",
    "    attn_map = np.zeros((grid_size * 28, grid_size * 28))\n",
    "    \n",
    "    for i in range(bag_size):\n",
    "        row = i // grid_size\n",
    "        col = i % grid_size\n",
    "        img = bag[i].squeeze().cpu().numpy()\n",
    "        composite_img[row*28:(row+1)*28, col*28:(col+1)*28] = img\n",
    "        attn_map[row*28:(row+1)*28, col*28:(col+1)*28] = attn_weights[i]\n",
    "    \n",
    "    # Visualizar la imagen base\n",
    "    ax.imshow(composite_img, cmap='gray', alpha=1.0)\n",
    "    \n",
    "    # Superponer mapa de calor de atención\n",
    "    cmap = LinearSegmentedColormap.from_list('custom_cmap', ['blue', 'yellow', 'red'])\n",
    "    heat = ax.imshow(attn_map, cmap=cmap, alpha=0.5)\n",
    "    \n",
    "    # Añadir barra de color\n",
    "    cbar = plt.colorbar(heat, ax=ax)\n",
    "    cbar.set_label('Peso de atención')\n",
    "    \n",
    "    # Título\n",
    "    correct = (bag_label.item() > 0.5) == (prediction > 0.5)\n",
    "    title = f\"Predicción {'correcta' if correct else 'incorrecta'}: {prediction:.2f}, Real: {bag_label.item()}\"\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Cell 4: Loop de entrenamiento con visualización\n",
    "def train():\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for bag, bag_label, inst_labels, lap in dataloader_train:\n",
    "            bag = bag.to(device)\n",
    "            bag_label = bag_label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, attn = model(bag)\n",
    "            loss = criterion(output.squeeze(-1), bag_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for bag, bag_label, inst_labels, lap in dataloader_val:\n",
    "                bag = bag.to(device)\n",
    "                bag_label = bag_label.to(device)\n",
    "                output, attn = model(bag)\n",
    "                loss = criterion(output.squeeze(-1), bag_label)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                pred = (output.squeeze(-1) > 0.5).float()\n",
    "                correct += (pred == bag_label).sum().item()\n",
    "                total += bag_label.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(dataloader_val)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Época {epoch}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Visualizar una bolsa de validación cada 2 épocas\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Obtener una muestra de validación\n",
    "                val_bag, val_label, val_inst_labels, val_lap = next(iter(dataloader_val))\n",
    "                val_bag = val_bag.to(device)\n",
    "                val_pred, val_attn = model(val_bag)\n",
    "                val_attn = val_attn.cpu().numpy()[0]\n",
    "                \n",
    "                # Visualizar la bolsa con pesos de atención\n",
    "                visualize_bag(\n",
    "                    val_bag.cpu(), \n",
    "                    val_label, \n",
    "                    val_inst_labels, \n",
    "                    val_lap,\n",
    "                    val_attn, \n",
    "                    title=f\"Época {epoch} - Bolsa de validación (pred={val_pred.item():.2f})\"\n",
    "                )\n",
    "    \n",
    "    # Visualizar curvas de pérdida\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Curvas de pérdida durante el entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Entrenar el modelo\n",
    "train()\n",
    "\n",
    "# Cell 5: Evaluación en test set con visualizaciones\n",
    "model.eval()\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "all_attentions = []\n",
    "all_bags = []\n",
    "all_inst_labels = []\n",
    "all_laps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bag, bag_label, inst_labels, lap in dataloader_test:\n",
    "        bag = bag.to(device)\n",
    "        bag_label = bag_label.to(device)\n",
    "        output, attn = model(bag)\n",
    "        \n",
    "        all_outputs.append(output.cpu())\n",
    "        all_labels.append(bag_label.cpu())\n",
    "        if pooling_type == 'attention':\n",
    "            all_attentions.append(attn.cpu())\n",
    "        \n",
    "        # Guardar las bolsas y etiquetas para visualizaciones\n",
    "        all_bags.append(bag.cpu())\n",
    "        all_inst_labels.append(inst_labels)\n",
    "        all_laps.append(lap)\n",
    "\n",
    "# Convertir listas a tensores\n",
    "torch_outputs = torch.cat(all_outputs).squeeze()\n",
    "torch_labels = torch.cat(all_labels)\n",
    "torch_preds = (torch_outputs > 0.5).float()\n",
    "\n",
    "# Métricas\n",
    "accuracy = (torch_preds == torch_labels).float().mean()\n",
    "print(f\"Accuracy en test: {accuracy:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(torch_labels.numpy(), torch_preds.numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negativo', 'Positivo'], \n",
    "            yticklabels=['Negativo', 'Positivo'])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, _ = roc_curve(torch_labels.numpy(), torch_outputs.numpy())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualización de casos correctos e incorrectos\n",
    "correct_indices = (torch_preds == torch_labels).nonzero().squeeze().numpy()\n",
    "incorrect_indices = (torch_preds != torch_labels).nonzero().squeeze().numpy()\n",
    "\n",
    "# Asegurarse de que los índices sean iterables incluso si solo hay uno\n",
    "if not hasattr(correct_indices, \"__len__\"):\n",
    "    correct_indices = [correct_indices]\n",
    "if not hasattr(incorrect_indices, \"__len__\"):\n",
    "    incorrect_indices = [incorrect_indices]\n",
    "\n",
    "print(f\"Predicciones correctas: {len(correct_indices)}\")\n",
    "print(f\"Predicciones incorrectas: {len(incorrect_indices)}\")\n",
    "\n",
    "# Visualizar casos correctos\n",
    "if len(correct_indices) > 0:\n",
    "    # Seleccionar hasta 3 casos correctos aleatorios\n",
    "    samples = np.random.choice(correct_indices, size=min(3, len(correct_indices)), replace=False)\n",
    "    for idx in samples:\n",
    "        bag = all_bags[idx]\n",
    "        bag_label = torch_labels[idx]\n",
    "        inst_labels = all_inst_labels[idx]\n",
    "        lap = all_laps[idx]\n",
    "        prediction = torch_outputs[idx].item()\n",
    "        \n",
    "        # Visualizar la bolsa\n",
    "        visualize_bag(\n",
    "            bag, \n",
    "            bag_label, \n",
    "            inst_labels, \n",
    "            lap,\n",
    "            all_attentions[idx][0].numpy() if pooling_type == 'attention' else None,\n",
    "            title=f\"Predicción CORRECTA: {prediction:.2f}, Real: {bag_label.item()}\"\n",
    "        )\n",
    "        \n",
    "        # Visualizar mapa de calor si estamos usando atención\n",
    "        if pooling_type == 'attention':\n",
    "            visualize_attention_heatmap(\n",
    "                bag,\n",
    "                all_attentions[idx][0].numpy(),\n",
    "                bag_label,\n",
    "                prediction\n",
    "            )\n",
    "\n",
    "# Visualizar casos incorrectos\n",
    "if len(incorrect_indices) > 0:\n",
    "    # Seleccionar hasta 3 casos incorrectos aleatorios\n",
    "    samples = np.random.choice(incorrect_indices, size=min(3, len(incorrect_indices)), replace=False)\n",
    "    for idx in samples:\n",
    "        bag = all_bags[idx]\n",
    "        bag_label = torch_labels[idx]\n",
    "        inst_labels = all_inst_labels[idx]\n",
    "        lap = all_laps[idx]\n",
    "        prediction = torch_outputs[idx].item()\n",
    "        \n",
    "        # Visualizar la bolsa\n",
    "        visualize_bag(\n",
    "            bag, \n",
    "            bag_label, \n",
    "            inst_labels, \n",
    "            lap,\n",
    "            all_attentions[idx][0].numpy() if pooling_type == 'attention' else None,\n",
    "            title=f\"Predicción INCORRECTA: {prediction:.2f}, Real: {bag_label.item()}\"\n",
    "        )\n",
    "        \n",
    "        # Visualizar mapa de calor si estamos usando atención\n",
    "        if pooling_type == 'attention':\n",
    "            visualize_attention_heatmap(\n",
    "                bag,\n",
    "                all_attentions[idx][0].numpy(),\n",
    "                bag_label,\n",
    "                prediction\n",
    "            )\n",
    "\n",
    "# Análisis de atención (específico para pooling de atención)\n",
    "if pooling_type == 'attention':\n",
    "    # Calcular correlación entre atención y presencia del dígito objetivo\n",
    "    print(\"\\nAnálisis de atención:\")\n",
    "    \n",
    "    # Recopilar datos para análisis\n",
    "    attn_on_target = []\n",
    "    attn_on_nontarget = []\n",
    "    \n",
    "    for idx in range(len(all_bags)):\n",
    "        bag_attn = all_attentions[idx][0].numpy()\n",
    "        inst_labels = all_inst_labels[idx].squeeze().numpy()\n",
    "        \n",
    "        for i in range(bag_size):\n",
    "            if inst_labels[i] == 1:  # Instancia con dígito objetivo\n",
    "                attn_on_target.append(bag_attn[i])\n",
    "            else:  # Instancia sin dígito objetivo\n",
    "                attn_on_nontarget.append(bag_attn[i])\n",
    "    \n",
    "    # Convertir a arrays de numpy\n",
    "    attn_on_target = np.array(attn_on_target)\n",
    "    attn_on_nontarget = np.array(attn_on_nontarget)\n",
    "    \n",
    "    # Estadísticas descriptivas\n",
    "    print(f\"Atención media en instancias con dígito {target_digit}: {attn_on_target.mean():.4f} ± {attn_on_target.std():.4f}\")\n",
    "    print(f\"Atención media en instancias sin dígito {target_digit}: {attn_on_nontarget.mean():.4f} ± {attn_on_nontarget.std():.4f}\")\n",
    "    \n",
    "    # Visualizar distribución de atención\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(attn_on_target, color='red', alpha=0.5, label=f'Dígito {target_digit}', kde=True)\n",
    "    sns.histplot(attn_on_nontarget, color='blue', alpha=0.5, label='Otros dígitos', kde=True)\n",
    "    plt.xlabel('Peso de atención')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.title('Distribución de pesos de atención por tipo de instancia')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualización de pesos de atención (independiente del tipo de pooling)\n",
    "# Obtenemos una bolsa específica para analizar\n",
    "sample_idx = 0  # Podemos cambiar este índice según interés\n",
    "bag, bag_label, inst_labels, lap = (\n",
    "    all_bags[sample_idx], \n",
    "    torch_labels[sample_idx], \n",
    "    all_inst_labels[sample_idx], \n",
    "    all_laps[sample_idx]\n",
    ")\n",
    "\n",
    "# Para pooling de atención, visualizamos los pesos de atención\n",
    "if pooling_type == 'attention':\n",
    "    attn_weights = all_attentions[sample_idx][0].numpy()\n",
    "    \n",
    "    # Visualizar pesos de atención como barras\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    bars = plt.bar(range(len(attn_weights)), attn_weights)\n",
    "    plt.xlabel('Índice de instancia')\n",
    "    plt.ylabel('Peso de atención')\n",
    "    plt.title('Pesos de atención para una bolsa de prueba')\n",
    "    \n",
    "    # Colorear las barras según si contienen el dígito objetivo\n",
    "    inst_labels_np = inst_labels.squeeze().numpy()\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color('red' if inst_labels_np[i] == 1 else 'blue')\n",
    "    \n",
    "    # Añadir leyenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='red', label=f'Dígito {target_digit}'),\n",
    "        Patch(facecolor='blue', label='Otros dígitos')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Obtener la bolsa completa para visualizar junto con la atención\n",
    "    visualize_bag(\n",
    "        bag, \n",
    "        bag_label, \n",
    "        inst_labels, \n",
    "        lap,\n",
    "        attn_weights,\n",
    "        title=f\"Análisis de atención - Dígito objetivo: {target_digit}\"\n",
    "    )\n",
    "\n",
    "# Para cualquier tipo de pooling, visualizamos la bolsa y su predicción\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        bag_tensor = bag.to(device)\n",
    "        output, _ = model(bag_tensor)\n",
    "        prediction = output.cpu().item()\n",
    "    \n",
    "    visualize_bag(\n",
    "        bag, \n",
    "        bag_label, \n",
    "        inst_labels, \n",
    "        lap,\n",
    "        title=f\"Predicción: {prediction:.2f} - Tipo de pooling: {pooling_type}\"\n",
    "    )\n",
    "\n",
    "# Comparación de métodos de pooling (si hay interés en comparar)\n",
    "def compare_pooling_methods():\n",
    "    \"\"\"\n",
    "    Comparar diferentes métodos de pooling en la misma bolsa.\n",
    "    \"\"\"\n",
    "    # Guardar los pesos originales del modelo con pooling actual\n",
    "    original_state_dict = model.state_dict()\n",
    "    \n",
    "    # Seleccionar una bolsa para análisis\n",
    "    bag, bag_label, inst_labels, lap = next(iter(dataloader_test))\n",
    "    \n",
    "    results = {}\n",
    "    pooling_types = ['max', 'mean', 'attention']\n",
    "    \n",
    "    for pool_type in pooling_types:\n",
    "        # Crear un modelo con el tipo de pooling específico\n",
    "        temp_model = MILModel(pooling_type=pool_type).to(device)\n",
    "        \n",
    "        # Si es el mismo tipo de pooling que el original, usar los pesos entrenados\n",
    "        if pool_type == pooling_type:\n",
    "            temp_model.load_state_dict(original_state_dict)\n",
    "        \n",
    "        # Evaluar\n",
    "        temp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            bag_tensor = bag.to(device)\n",
    "            output, attn = temp_model(bag_tensor)\n",
    "            prediction = output.cpu().item()\n",
    "        \n",
    "        results[pool_type] = {\n",
    "            'prediction': prediction,\n",
    "            'attention': attn.cpu().numpy()[0] if pool_type == 'attention' else None\n",
    "        }\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, pool_type in enumerate(pooling_types):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(f\"Pooling: {pool_type}\\nPred: {results[pool_type]['prediction']:.2f}\")\n",
    "        \n",
    "        # La bolsa es la misma, solo mostramos una imagen representativa\n",
    "        representative_idx = 0\n",
    "        img = bag[0, representative_idx, 0].numpy()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Comparación de métodos de pooling - Etiqueta real: {bag_label.item()}\")\n",
    "    plt.subplots_adjust(top=0.8)\n",
    "    plt.show()\n",
    "    \n",
    "    # Restaurar el modelo original\n",
    "    model.load_state_dict(original_state_dict)\n",
    "\n",
    "# Ejecutar comparación de pooling si se desea\n",
    "compare_pooling_methods()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
